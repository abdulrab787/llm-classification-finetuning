{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ffad45",
   "metadata": {},
   "source": [
    "# 03 - Model Baseline\n",
    "This notebook implements a baseline model for text classification using transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c49a6",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7f24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506953c5",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c3f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (45981, 10)\n",
      "Val shape: (11496, 10)\n",
      "Test shape: (3, 4)\n",
      "\n",
      "Train columns: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'target']\n"
     ]
    }
   ],
   "source": [
    "# Load processed data\n",
    "train_df = pd.read_csv('../data/train_processed.csv')\n",
    "val_df = pd.read_csv('../data/val_processed.csv')\n",
    "test_df = pd.read_csv('../data/test_processed.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Val shape: {val_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTrain columns: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Create combined text column from prompt and responses for training/validation\n",
    "# Using all three columns concatenated\n",
    "if 'prompt_clean' in train_df.columns:\n",
    "    train_df['text'] = train_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                       train_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                       train_df['response_b_clean'].fillna('')\n",
    "    val_df['text'] = val_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                     val_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                     val_df['response_b_clean'].fillna('')\n",
    "    test_df['text'] = test_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                      test_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                      test_df['response_b_clean'].fillna('')\n",
    "    print(\"Combined text columns created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cbb31",
   "metadata": {},
   "source": [
    "## Initialize Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ede1e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: bert-base-uncased\n",
      "Number of labels: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "# Use 'target' column for labels (0: Model A wins, 1: Model B wins, 2: Tie)\n",
    "num_labels = 3  # We have 3 classes: Model A wins, Model B wins, Tie\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "print(f\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cf465",
   "metadata": {},
   "source": [
    "## Create Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6af1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset class defined.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "print(\"Custom Dataset class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff863f7",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4b1f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Train dataset size: 45981\n",
      "Val dataset size: 11496\n"
     ]
    }
   ],
   "source": [
    "# Ensure data is loaded\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "if 'train_df' not in locals():\n",
    "    train_df = pd.read_csv('../data/train_processed.csv')\n",
    "    val_df = pd.read_csv('../data/val_processed.csv')\n",
    "    test_df = pd.read_csv('../data/test_processed.csv')\n",
    "    \n",
    "    # Create combined text column if it doesn't exist\n",
    "    if 'text' not in train_df.columns:\n",
    "        # Try to use cleaned columns if available, otherwise use original columns\n",
    "        if 'prompt_clean' in train_df.columns:\n",
    "            train_df['text'] = train_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_b_clean'].fillna('')\n",
    "            val_df['text'] = val_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_b_clean'].fillna('')\n",
    "            test_df['text'] = test_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                              test_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                              test_df['response_b_clean'].fillna('')\n",
    "        else:\n",
    "            # Use original columns if cleaned versions don't exist\n",
    "            train_df['text'] = train_df['prompt'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_a'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_b'].fillna('')\n",
    "            val_df['text'] = val_df['prompt'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_a'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_b'].fillna('')\n",
    "            test_df['text'] = test_df['prompt'].fillna('') + ' [SEP] ' + \\\n",
    "                              test_df['response_a'].fillna('') + ' [SEP] ' + \\\n",
    "                              test_df['response_b'].fillna('')\n",
    "    print(\"Data loaded.\")\n",
    "\n",
    "# Ensure tokenizer is available\n",
    "if 'tokenizer' not in locals():\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Define ClassificationDataset class\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ClassificationDataset(\n",
    "    train_df['text'].tolist(),\n",
    "    train_df['target'].tolist() if 'target' in train_df.columns else None,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = ClassificationDataset(\n",
    "    val_df['text'].tolist(),\n",
    "    val_df['target'].tolist() if 'target' in val_df.columns else None,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab4cef",
   "metadata": {},
   "source": [
    "## Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9041efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured.\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary imports\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/baseline',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f8dcd",
   "metadata": {},
   "source": [
    "## Define Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c71dcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics function defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print(\"Metrics function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dfcf7",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47159b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Ensure training_args are available\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/baseline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[32m    106\u001b[39m \n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    109\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining arguments set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Ensure compute_metrics function is defined\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# Ensure necessary imports and variables\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Trainer, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Ensure data is loaded\n",
    "if 'train_df' not in locals():\n",
    "    train_df = pd.read_csv('../data/train_processed.csv')\n",
    "    val_df = pd.read_csv('../data/val_processed.csv')\n",
    "    test_df = pd.read_csv('../data/test_processed.csv')\n",
    "    \n",
    "    # Create combined text column if it doesn't exist\n",
    "    if 'text' not in train_df.columns:\n",
    "        if 'prompt_clean' in train_df.columns:\n",
    "            train_df['text'] = train_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_b_clean'].fillna('')\n",
    "            val_df['text'] = val_df['prompt_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_a_clean'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_b_clean'].fillna('')\n",
    "        else:\n",
    "            train_df['text'] = train_df['prompt'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_a'].fillna('') + ' [SEP] ' + \\\n",
    "                               train_df['response_b'].fillna('')\n",
    "            val_df['text'] = val_df['prompt'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_a'].fillna('') + ' [SEP] ' + \\\n",
    "                             val_df['response_b'].fillna('')\n",
    "    print(\"Data loaded.\")\n",
    "\n",
    "# Ensure tokenizer is available\n",
    "if 'tokenizer' not in locals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Define ClassificationDataset class\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Ensure datasets are created\n",
    "if 'train_dataset' not in locals():\n",
    "    train_dataset = ClassificationDataset(\n",
    "        train_df['text'].tolist(),\n",
    "        train_df['target'].tolist() if 'target' in train_df.columns else None,\n",
    "        tokenizer\n",
    "    )\n",
    "    val_dataset = ClassificationDataset(\n",
    "        val_df['text'].tolist(),\n",
    "        val_df['target'].tolist() if 'target' in val_df.columns else None,\n",
    "        tokenizer\n",
    "    )\n",
    "    print(f\"Datasets created. Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Ensure model is available\n",
    "if 'model' not in locals():\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "    print(f\"Model loaded.\")\n",
    "\n",
    "# Ensure training_args are available\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/baseline\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "\n",
    "# Ensure compute_metrics function is defined\n",
    "if 'compute_metrics' not in locals():\n",
    "    def compute_metrics(eval_preds):\n",
    "        predictions, labels = eval_preds\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "        precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "        recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d5900",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if not key.startswith('runtime'):\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74151d1b",
   "metadata": {},
   "source": [
    "## Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset without labels\n",
    "test_dataset = ClassificationDataset(\n",
    "    test_df['text'].tolist(),\n",
    "    labels=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "predictions = trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "print(f\"Predictions shape: {test_preds.shape}\")\n",
    "print(f\"Unique predictions: {np.unique(test_preds)}\")\n",
    "print(f\"Prediction mapping: 0=Model A wins, 1=Model B wins, 2=Tie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f901b",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2996a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = test_df[['id']].copy() if 'id' in test_df.columns else test_df.copy()\n",
    "submission_df['prediction'] = test_preds\n",
    "\n",
    "# Map predictions to labels\n",
    "label_map = {0: 'Model A wins', 1: 'Model B wins', 2: 'Tie'}\n",
    "submission_df['prediction_label'] = submission_df['prediction'].map(label_map)\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv('../submissions/submission.csv', index=False)\n",
    "\n",
    "print(f\"Submission saved to ../submissions/submission.csv\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc71d07",
   "metadata": {},
   "source": [
    "## Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ceee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "print(\"Saving fine-tuned model...\")\n",
    "trainer.save_model('../models/baseline')\n",
    "tokenizer.save_pretrained('../models/baseline')\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n",
    "print(\"Model saved to: ../models/baseline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
